{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeeeb7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (1.21.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: yfinance in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (0.2.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: requests>=2.26 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from yfinance) (2.28.2)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from yfinance) (2.3.5)\n",
      "Requirement already satisfied: html5lib>=1.1 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from yfinance) (4.11.1)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: cryptography>=3.3.2 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from yfinance) (37.0.1)\n",
      "Requirement already satisfied: lxml>=4.9.1 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from yfinance) (4.9.1)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from yfinance) (1.4.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.3.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from cryptography>=3.3.2->yfinance) (1.15.1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (1.26.11)\n",
      "Requirement already satisfied: pycparser in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=3.3.2->yfinance) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pandas scikit-learn yfinance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15789f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Mean Squared Error: 2.0679515313825692e-25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Fetch stock data from Yahoo Finance\n",
    "def get_stock_data(ticker, period='1y', interval='1d'):\n",
    "    # Download the stock price data\n",
    "    data = yf.download(ticker, period=period, interval=interval)\n",
    "    data['Date'] = data.index\n",
    "    return data\n",
    "\n",
    "# Prepare the dataset for training the model\n",
    "def prepare_data(df, feature_columns, target_column, test_size=0.2):\n",
    "    X = df[feature_columns]\n",
    "    y = df[target_column]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Define the ticker symbol for an Indian stock and the time period for the data\n",
    "ticker = 'RELIANCE.NS'  # Example for Reliance Industries Limited\n",
    "period = '1y'\n",
    "interval = '1d'\n",
    "\n",
    "# Fetch the stock data\n",
    "stock_data = get_stock_data(ticker, period, interval)\n",
    "\n",
    "# Feature engineering: Use only 'Open', 'High', 'Low', 'Close' for simplicity\n",
    "feature_columns = ['Open', 'High', 'Low', 'Close']\n",
    "target_column = 'Close'\n",
    "\n",
    "# Prepare the data\n",
    "X_train, X_test, y_train, y_test = prepare_data(stock_data, feature_columns, target_column)\n",
    "\n",
    "# Create and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Predict future prices\n",
    "# future_prices = model.predict(future_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "316c2a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (1.21.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: yfinance in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (0.2.12)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from yfinance) (2.3.5)\n",
      "Requirement already satisfied: html5lib>=1.1 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: cryptography>=3.3.2 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from yfinance) (37.0.1)\n",
      "Requirement already satisfied: lxml>=4.9.1 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from yfinance) (4.9.1)\n",
      "Requirement already satisfied: requests>=2.26 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from yfinance) (2.28.2)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from yfinance) (1.4.4)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from yfinance) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.3.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from cryptography>=3.3.2->yfinance) (1.15.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (3.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=3.3.2->yfinance) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pandas scikit-learn yfinance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "012da79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Mean Squared Error: 45042.60543141834\n",
      "R^2 Score: 0.9949956830101939\n",
      "Mean Squared Error: 383212.80687513034\n",
      "R^2 Score: 0.9960346032469262\n",
      "Mean Squared Error: 74.74217867558829\n",
      "R^2 Score: 0.9963794414342559\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to download stock data\n",
    "def get_stock_data(ticker, start_date, end_date):\n",
    "    data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    data.reset_index(inplace=True)\n",
    "    return data\n",
    "\n",
    "# Function to prepare features and target variables\n",
    "def feature_target_split(data, target_column, n_lag_days=5):\n",
    "    # Create lag features\n",
    "    for i in range(1, n_lag_days + 1):\n",
    "        data[f'lag_{i}'] = data[target_column].shift(i)\n",
    "    data = data.dropna()\n",
    "    X = data[[f'lag_{i}' for i in range(1, n_lag_days + 1)]]\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Function to create and evaluate a model\n",
    "def create_and_evaluate_model(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"R^2 Score: {r2}\")\n",
    "    return model\n",
    "\n",
    "# Get data for Nifty and Sensex\n",
    "nifty_data = get_stock_data(\"^NSEI\", \"2020-01-01\", \"2023-01-01\")\n",
    "sensex_data = get_stock_data(\"^BSESN\", \"2020-01-01\", \"2023-01-01\")\n",
    "\n",
    "# Get data for Tata Motors\n",
    "tata_motors_data = get_stock_data(\"TATAMOTORS.NS\", \"2020-01-01\", \"2023-01-01\")\n",
    "\n",
    "# Prepare the data\n",
    "nifty_X, nifty_y = feature_target_split(nifty_data, 'Close')\n",
    "sensex_X, sensex_y = feature_target_split(sensex_data, 'Close')\n",
    "tata_motors_X, tata_motors_y = feature_target_split(tata_motors_data, 'Close')\n",
    "\n",
    "# Split the data into training and testing datasets\n",
    "nifty_X_train, nifty_X_test, nifty_y_train, nifty_y_test = train_test_split(nifty_X, nifty_y, test_size=0.2, random_state=42)\n",
    "sensex_X_train, sensex_X_test, sensex_y_train, sensex_y_test = train_test_split(sensex_X, sensex_y, test_size=0.2, random_state=42)\n",
    "tata_motors_X_train, tata_motors_X_test, tata_motors_y_train, tata_motors_y_test = train_test_split(tata_motors_X, tata_motors_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and evaluate models\n",
    "nifty_model = create_and_evaluate_model(nifty_X_train, nifty_X_test, nifty_y_train, nifty_y_test)\n",
    "sensex_model = create_and_evaluate_model(sensex_X_train, sensex_X_test, sensex_y_train, sensex_y_test)\n",
    "tata_motors_model = create_and_evaluate_model(tata_motors_X_train, tata_motors_X_test, tata_motors_y_train, tata_motors_y_test)\n",
    "\n",
    "# Make future predictions (Example for Tata Motors)\n",
    "# future_data = ...\n",
    "# tata_motors_predictions = tata_motors_model.predict(future_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "699b1a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Accuracy: 0.52\n",
      "Trade Decision for Tomorrow: Sell\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saharsh\\anaconda3\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Download historical data for a stock\n",
    "ticker = 'AAPL'  # You can change this to any stock of your choice\n",
    "data = yf.download(ticker, start='2020-01-01', end='2023-01-01')\n",
    "\n",
    "# Feature Engineering: Calculate daily returns and other technical indicators\n",
    "data['Return'] = data['Close'].pct_change()\n",
    "data['MA10'] = data['Close'].rolling(window=10).mean()\n",
    "data['MA50'] = data['Close'].rolling(window=50).mean()\n",
    "data['Volume_Change'] = data['Volume'].pct_change()\n",
    "data['Volatility'] = data['Return'].rolling(window=10).std()\n",
    "\n",
    "# Target Variable: Whether the stock will go up (1) or down (0) the next day\n",
    "data['Target'] = np.where(data['Return'].shift(-1) > 0, 1, 0)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "data = data.dropna()\n",
    "\n",
    "# Prepare features and target\n",
    "features = ['Return', 'MA10', 'MA50', 'Volume_Change', 'Volatility']\n",
    "X = data[features]\n",
    "y = data['Target']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Example: Making a trade decision for the next day\n",
    "# If the prediction is 1, we buy; if it's 0, we sell/short\n",
    "today_features = np.array([[\n",
    "    data.iloc[-1]['Return'],\n",
    "    data.iloc[-1]['MA10'],\n",
    "    data.iloc[-1]['MA50'],\n",
    "    data.iloc[-1]['Volume_Change'],\n",
    "    data.iloc[-1]['Volatility']\n",
    "]])\n",
    "trade_decision = model.predict(today_features)\n",
    "print('Trade Decision for Tomorrow:', 'Buy' if trade_decision[0] == 1 else 'Sell')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "409f7286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Accuracy: 0.54\n",
      "Trade Decision for Tomorrow: Sell\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saharsh\\anaconda3\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Download historical data for Tata Steel\n",
    "ticker = 'TATASTEEL.NS'  # Ticker symbol for Tata Steel on NSE\n",
    "data = yf.download(ticker, start='2020-01-01', end='2023-01-01')\n",
    "\n",
    "# Feature Engineering: Calculate daily returns and other technical indicators\n",
    "data['Return'] = data['Close'].pct_change()\n",
    "data['MA10'] = data['Close'].rolling(window=10).mean()\n",
    "data['MA50'] = data['Close'].rolling(window=50).mean()\n",
    "data['Volume_Change'] = data['Volume'].pct_change()\n",
    "data['Volatility'] = data['Return'].rolling(window=10).std()\n",
    "\n",
    "# Target Variable: Whether the stock will go up (1) or down (0) the next day\n",
    "data['Target'] = np.where(data['Return'].shift(-1) > 0, 1, 0)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "data = data.dropna()\n",
    "\n",
    "# Prepare features and target\n",
    "features = ['Return', 'MA10', 'MA50', 'Volume_Change', 'Volatility']\n",
    "X = data[features]\n",
    "y = data['Target']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Example: Making a trade decision for the next day\n",
    "# If the prediction is 1, we buy; if it's 0, we sell/short\n",
    "today_features = np.array([[\n",
    "    data.iloc[-1]['Return'],\n",
    "    data.iloc[-1]['MA10'],\n",
    "    data.iloc[-1]['MA50'],\n",
    "    data.iloc[-1]['Volume_Change'],\n",
    "    data.iloc[-1]['Volatility']\n",
    "]])\n",
    "trade_decision = model.predict(today_features)\n",
    "print('Trade Decision for Tomorrow:', 'Buy' if trade_decision[0] == 1 else 'Sell')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20ed24cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (1.21.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: yfinance in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (0.2.12)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: requests>=2.26 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from yfinance) (2.28.2)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: html5lib>=1.1 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from yfinance) (1.4.4)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from yfinance) (2.3.5)\n",
      "Requirement already satisfied: lxml>=4.9.1 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from yfinance) (4.9.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from yfinance) (4.11.1)\n",
      "Requirement already satisfied: cryptography>=3.3.2 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from yfinance) (37.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.3.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from cryptography>=3.3.2->yfinance) (1.15.1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (3.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=3.3.2->yfinance) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pandas scikit-learn yfinance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6d51ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Mean Squared Error: 14.359597352112505\n",
      "Cross-validated scores: [-0.69409989  0.6135132   0.60856603  0.60292092  0.14462653]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Fetch historical data for a stock (using Tata Steel as an example)\n",
    "ticker = 'TATASTEEL.NS'\n",
    "data = yf.download(ticker, start='2018-01-01', end='2023-01-01')\n",
    "\n",
    "# Feature Engineering\n",
    "# Calculate technical indicators like moving averages, RSI, etc.\n",
    "data['SMA_20'] = data['Close'].rolling(window=20).mean()\n",
    "data['SMA_50'] = data['Close'].rolling(window=50).mean()\n",
    "data['Volatility'] = data['Close'].rolling(window=20).std()\n",
    "\n",
    "# Shift the closing price forward to predict future prices\n",
    "forecast_days = 5\n",
    "data['Future_Close'] = data['Close'].shift(-forecast_days)\n",
    "\n",
    "# Drop NaN values created by rolling windows and shifting\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Features and target\n",
    "X = data[['SMA_20', 'SMA_50', 'Volatility', 'Volume']]\n",
    "y = data['Future_Close']\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model initialization and training\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(model, X_scaled, y, cv=5)\n",
    "\n",
    "# Model evaluation\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Cross-validated scores: {cv_scores}\")\n",
    "\n",
    "# Making predictions\n",
    "# The output can be used for investment decisions, risk management, or further analysis\n",
    "predicted_prices = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8ebcbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Mean Squared Error: 14.359597352112505\n",
      "Cross-validated scores: [-0.69409989  0.6135132   0.60856603  0.60292092  0.14462653]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import yfinance as yf \n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "ticker = 'TATASTEEL.NS'\n",
    "data = yf.download(ticker, start='2018-01-01', end='2023-01-01')\n",
    "data['SMA_20'] = data['Close'].rolling(window=20).mean()\n",
    "data['SMA_50'] = data['Close'].rolling(window=50).mean()\n",
    "data['Volatility'] = data['Close'].rolling(window=20).std()\n",
    "forecast_days = 5 \n",
    "data['Future_Close'] = data['Close'].shift(-forecast_days)\n",
    "data.dropna(inplace=True)\n",
    "X = data[['SMA_20', 'SMA_50', 'Volatility', 'Volume']]\n",
    "y = data['Future_Close']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(model, X_scaled, y, cv=5)\n",
    "\n",
    "# Model evaluation\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Cross-validated scores: {cv_scores}\")\n",
    "\n",
    "# Making predictions\n",
    "# The output can be used for investment decisions, risk management, or further analysis\n",
    "predicted_prices = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07253a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (4.35.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.3.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.3.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saharsh\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "634a2bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the following sentences from English to French.\n",
      "Hello, how are you? - Bonjour, comment ça va?\n",
      "What is your name? - Comment vous appelez-vous?\n",
      "I am Bonjour\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "def generate_prompt(prompt_text, model_name='gpt2', max_length=50, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generate text based on a prompt using a pre-trained model.\n",
    "\n",
    "    :param prompt_text: The prompt text to feed into the model.\n",
    "    :param model_name: The pre-trained model to use.\n",
    "    :param max_length: The maximum length of the generated sequence.\n",
    "    :param temperature: The sampling temperature to use.\n",
    "    :return: The generated text.\n",
    "    \"\"\"\n",
    "\n",
    "    generator = pipeline('text-generation', model=model_name)\n",
    "    set_seed(42)\n",
    "    generated_text = generator(prompt_text, max_length=max_length, temperature=temperature)[0]['generated_text']\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "prompt_description = \"Translate the following sentences from English to French.\"\n",
    "prompt_examples = [\n",
    "    \"Hello, how are you? - Bonjour, comment ça va?\",\n",
    "    \"What is your name? - Comment vous appelez-vous?\"\n",
    "]\n",
    "\n",
    "# Create the prompt\n",
    "prompt_text = f\"{prompt_description}\\n\"\n",
    "for example in prompt_examples:\n",
    "    prompt_text += f\"{example}\\n\"\n",
    "\n",
    "# Generate the response\n",
    "response = generate_prompt(prompt_text)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec2b200a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14760\\2722843902.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0msource_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Hello, how are you?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# Translate from English to French\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mtranslated_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"en\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Translated text: {translated_text}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14760\\2722843902.py\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(text, src_language, target_language)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Initialize the tokenizer and model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMarianTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMarianMTModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"_from_config\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1250\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1251\u001b[1;33m         \u001b[0mrequires_backends\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backends\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py\u001b[0m in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[0mfailed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mavailable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchecks\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mavailable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfailed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfailed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: \nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "def translate(text, src_language=\"en\", target_language=\"fr\"):\n",
    "    \"\"\"\n",
    "    Translate text from a source language to a target language using the Helsinki-NLP models.\n",
    "    \n",
    "    :param text: The text to translate.\n",
    "    :param src_language: The source language (ISO 639-1 code).\n",
    "    :param target_language: The target language (ISO 639-1 code).\n",
    "    :return: The translated text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the model repository path\n",
    "    model_name = f'Helsinki-NLP/opus-mt-{src_language}-{target_language}'\n",
    "\n",
    "    # Initialize the tokenizer and model\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize the text\n",
    "    translated = model.generate(**tokenizer(text, return_tensors=\"pt\", padding=True))\n",
    "\n",
    "    # Decode the tokenized text\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "    return translated_text\n",
    "\n",
    "# Example usage\n",
    "source_text = \"Hello, how are you?\"\n",
    "# Translate from English to French\n",
    "translated_text = translate(source_text, \"en\", \"fr\")\n",
    "print(f\"Translated text: {translated_text}\")\n",
    "\n",
    "# To translate to other languages, you just need to change the target_language parameter\n",
    "# to the ISO 639-1 code of the target language. For example, for Spanish it would be \"es\", for German \"de\", etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33728a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp39-cp39-win_amd64.whl (977 kB)\n",
      "     -------------------------------------- 977.6/977.6 kB 6.2 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff2a8095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saharsh\\anaconda3\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text: Bonjour, comment allez-vous?\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "def translate(text, src_language=\"en\", target_language=\"fr\"):\n",
    "    \"\"\"\n",
    "    Translate text from a source language to a target language using the Helsinki-NLP models.\n",
    "    \n",
    "    :param text: The text to translate.\n",
    "    :param src_language: The source language (ISO 639-1 code).\n",
    "    :param target_language: The target language (ISO 639-1 code).\n",
    "    :return: The translated text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the model repository path\n",
    "    model_name = f'Helsinki-NLP/opus-mt-{src_language}-{target_language}'\n",
    "\n",
    "    # Initialize the tokenizer and model\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize the text\n",
    "    translated = model.generate(**tokenizer(text, return_tensors=\"pt\", padding=True))\n",
    "\n",
    "    # Decode the tokenized text\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "    return translated_text\n",
    "\n",
    "# Example usage\n",
    "source_text = \"Hello, how are you?\"\n",
    "# Translate from English to French\n",
    "translated_text = translate(source_text, \"en\", \"fr\")\n",
    "print(f\"Translated text: {translated_text}\")\n",
    "\n",
    "# To translate to other languages, you just need to change the target_language parameter\n",
    "# to the ISO 639-1 code of the target language. For example, for Spanish it would be \"es\", for German \"de\", etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8666171c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7fe3abee1d485a8fb493c799fa6a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saharsh\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\saharsh\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c153b4396754316932781ffaadfd76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/source.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0bfaf057ea4e928b3a4f446874641f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09484beb3b8f46c9b15f94701f211b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24cb670109234920a448f6a5f3d1924d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saharsh\\anaconda3\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b4cd2e6b984a359db0845eeb95cb23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c20a3a624b0406c83269f3911cbd132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text: Bonjour, comment allez-vous?\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "def translate(text, src_language=\"en\", target_language=\"fr\"):\n",
    "    \"\"\"\n",
    "    Translate text from a source language to a target language using the Helsinki-NLP models.\n",
    "    \n",
    "    :param text: The text to translate.\n",
    "    :param src_language: The source language (ISO 639-1 code).\n",
    "    :param target_language: The target language (ISO 639-1 code).\n",
    "    :return: The translated text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the model repository path\n",
    "    model_name = f'Helsinki-NLP/opus-mt-{src_language}-{target_language}'\n",
    "\n",
    "    # Initialize the tokenizer and model\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize the text\n",
    "    translated = model.generate(**tokenizer(text, return_tensors=\"pt\", padding=True))\n",
    "\n",
    "    # Decode the tokenized text\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "    return translated_text\n",
    "\n",
    "# Example usage\n",
    "source_text = \"Hello, how are you?\"\n",
    "# Translate from English to French\n",
    "translated_text = translate(source_text, \"en\", \"fr\")\n",
    "print(f\"Translated text: {translated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e448039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
